NAME: Stewart Dulaney
EMAIL: sdulaney@ucla.edu
ID: 904-064-791

Included files:

QUESTION 2.1.1 - causing conflicts:
- Why does it take many iterations before errors are seen?
The more iterations performed by each thread, the greater the processing time of the shared variableby an individual thread, and the more likely that the thread will be preempted by the thread scheduler before it is finished and while it is executing a critical section. As a result, with many iterations it is more likely that multiple threads are executing the critical section in parallel, and more likely that this race condition leaves the program in an invalid state (i.e., counter is not 0 when the program ends).
- Why does a significantly smaller number of iterations so seldom fail?
For a small enough number of iterations a thread is likely to finish its' processing of the shared variable before the next thread is created and the thread scheduler has a chance to preempt the first thread and context switch to the next thread (i.e., before the end of the time slice). As a result, the likelihood of a race condition is lower and the likelihood of all iterations for a single thread being an atomic operation is higher.

QUESTION 2.1.2 - cost of yielding:
- Why are the --yield runs so much slower?
Since each thread calls sched_yield() every time the counter is incremented or decremented, it relinquishes the CPU and the thread is moved to the end of the scheduling queue, and (assuming multiple threads) a new thread gets to run. Although not as expensive as a process context switch, a thread context switch is still a costly operation. Since we do an expensive thread context switch 2 * num_threads * num_iterations times and other threads are allowed to run on the CPU, the --yield runs are much slower.
- Where is the additional time going?
The additional time is going toward thread context switches which can require switching in and out of kernel mode and saving/restoring register state. There could also time spent by the thread scheduler choosing which thread will run next. Last, when a thread yields, the time other threads use the CPU is included in the total run time of the original thread.
- Is it possible to get valid per-operation timings if we are using the --yield option?
No.
- If so, explain how. If not, explain why not.
It is not possible because we can't accurately track the additional time spent on context switches and running other threads. Even if we tried to change our implementation so that a thread subtracted the time from when it called sched_yield() until it reached the next line of code, the thread could still be preempted by the thread scheduler at any time and we'd have no way of accounting for the time that passed until it was the thread's turn to run again.

QUESTION 2.1.3 - measurement errors:
- Why does the average cost per operation drop with increasing iterations?
The average cost per operation drops because as the number of iterations increases, the overhead associated with thread creation becomes less significant as compared with the time spent running all iterations. This is because the cost of thread creation is shared among all iterations and amortized by a large number of iterations, bringing the average cost per operation down.
- If the cost per iteration is a function of the number of iterations, how do we know how many iterations to run (or what the "correct" cost is)?
We can see from the plot that the average cost per iteration decreases exponentially, and thus will eventually become stable. Once the curve is not decreasing significantly and displays some stability, the number of iterations at that point would be a good choice for achieving an estimate of the "correct" cost per iteration.

QUESTION 2.1.4 - costs of serialization:
- Why do all of the options perform similarly for low numbers of threads?
For small numbers of threads, race conditions and lock contention don't happen as often often so the overhead of synchronization is not as significant for the runs that use mutexes, spin-locks, and compare-and-swap. As such, they perform similar to the unprotected run.
- Why do the three protected operations slow down as the number of threads rises?
As the number of threads grows, race conditions and lock contention happen more often so the overhead of synchronization is higher. Each thread spends more time waiting for locks to be released so the average time per operation increases.

QUESTION 2.2.1 - scalability of Mutex:
- Compare the variation in time per mutex-protected operation vs the number of threads in Part-1 (adds) and Part-2 (sorted lists).
As the number of threads increases, in Part-1 (adds) we see a slight decrease in time per mutex-protected operation, and in Part-2 (sorted lists) we see almost no change.
- Comment on the general shapes of the curves, and explain why they have this shape.
The curve for Part-1 is close to linear and although it was unexpected to me, it has a small negative slope. It's possible that increasing the number of threads continues to amortize the cost of thread creation, with this decrease in cost being greater than the cost of increased lock contention. Since threads waiting on a mutex lock aren't run by the CPU until the lock is available, this could explain why the overhead of increased lock contention is less significant. One could also hypothesize that external factors such as a busy server could explain this shape. For Part-2 the curve is more or less flat with no slope. This could be explained by the entire linked list being locked while a single thread inserts, checks the length, looks up elements, and deletes them. These O(n) operations are more expensive than simple addition, so the locks in Part-2 stay locked for longer periods and as a result there is less overhead added for context switching as the number of threads increases.
- Comment on the relative rates of increase and differences in the shapes of the curves, and offer an explanation for these differences.
This was answered in detail for the previous question.

QUESTION 2.2.2 - scalability of spin locks:
- Compare the variation in time per protected operation vs the number of threads for list operations protected by Mutex vs Spin locks. Comment on the general shapes of the curves, and explain why they have this shape.
The curve for mutex-protected list operations is relatively flat as previously discussed, with notable reasons being because threads are put to sleep until a lock is available so they don't contribute to total run time and because less context switches occur when the entire linked list is locked for a long period. The curve for spin lock protected list operations is more or less linear with a positive slope which can be attributed to more CPU cycles being spent checking to see if the lock is available as the number of threads (and lock contention) increases.
- Comment on the relative rates of increase and differences in the shapes of the curves, and offer an explanation for these differences.
The cost per list operation for spin locks grows much faster than that of mutexes, which is relatively stable. This is because spin locks waste a lot of time checking to see if the lock is available when more threads are competing for the lock. On the other hand, with mutexes the threads are put to sleep if a section is locked so contention doesn't result in as many wastes cycles. The cost is about the same for both for a small number of threads. This is because without less lock contention the CPU wastes less time on the spinning threads.
