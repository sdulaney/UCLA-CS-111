NAME: Stewart Dulaney
EMAIL: sdulaney@ucla.edu
ID: 904-064-791

Included files:


QUESTION 2.3.1 - Cycles in the basic list implementation:
- Where do you believe most of the cycles are spent in the 1 and 2-thread list tests?
In the 1 and 2-thread list tests, it seems reasonable to assume that most of the cycles are spent actually performing the list operations in the critical section.
- Why do you believe these to be the most expensive parts of the code?
If there are only 1 or 2 threads, there is less lock contention and so it seems reasonable to assume that less execution time (compared with high-thread scenarios) is being spent getting the locks. On the other hand, list operations on a sorted linked list are O(n) in the size of the list and that processing time would be the most expensive if synchronization overhead is low.
- Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
In high-thread spin-lock tests, most of the cycles would be spent spinning and checking if the lock is available if lock contention is high enough.
- Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?
In high-thread mutex tests, most of the time/cycles are spent on either context switches or list operations. While context switches are very expensive, more cycles could be spent on list operations if the list was long enough (linear time operations). On the other hand, if the list is short, more cycles would be spent on switching from user mode to kernel mode and back for a context switch.

QUESTION 2.3.2 - Execution Profiling:
- Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?
When the spin-lock version of the list exerciser is run with a large number of threads, most of the cycles are consumed by the lines of code checking if the spin lock is available (i.e., the while loop that spins and runs __sync_lock_test_and_set until the lock is available):
while (__sync_lock_test_and_set(&spin_lock, 1)) {
      continue;
}
This accounts for 408/458 * 100 = 89.1% of samples, while the remaining (only) 10.9% are running SortedList_lookup.
- Why does this operation become so expensive with large numbers of threads?
With large numbers of threads this operation becomes very expensive because with high lock contention all other threads spend cycles spinning and checking the spin lock until a single thread finishes all of its work on the list (the critical section). So threads spend a lot of time spinning in the loop above, waiting to get the lock for the critical section.

QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
- Why does the average lock-wait time rise so dramatically with the number of contending threads?
As the number of contending threads increases, it is more likely that the lock will be unavailable when a thread needs it. In addition, a pool/queue of multiple threads may be waiting on the same lock, causing the dramatic increase in time to get a lock. For example, a thread could have to wait while all other threads get the lock and execute their critical section first, depending on the algorithm for contending threads.
- Why does the completion time per operation rise (less dramatically) with the number of contending threads?
As lock-wait time increases with the number of contending threads, time per operation must also rise because the time to execute the critical section stays the same. The increased overhead of mutex synchronization and the increase in lock-wait time are responsible for the increased completion time per operation.
- How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?
The completion time per operation is calculated in the parent thread using wall time. On the other hand, lock-wait time per operation is calculated in each individual thread created by the parent thread. The time periods of waiting of individual threads can overlap when multiple threads are waiting at the same time, which explains how the lock-wait time operation can go up faster/higher than the completion time per operation. When a nanosecond is added to the total completion time, multiple nanoseconds can be added to the total mutex wait time when multiple threads are waiting.

QUESTION 2.3.4 - Performance of Partitioned Lists
- Explain the change in performance of the synchronized methods as a function of the number of lists.
As the number of lists increases the throughput or total number of operations per second also increases. TODO
- Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.

- It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.